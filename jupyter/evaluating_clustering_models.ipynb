{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Title: Evaluating Clustering Models\n",
    "Date: 2019-04-21 10:20\n",
    "Tags: python\n",
    "Slug: blog-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This week I looked into some model evaluation metrics for clustering methods, which I'm going to summarize here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two situations when we are dealing with clustering model evaluation. The first one is simple, which is when our data has the ground truth attached to them (i.e., we have the true labels for group membership). In such case there are quite a few metrics we can use for model comparison. However, when our data doesn't have true labels, things become a little more complicated, but there is still some way which I'll talk about in the second part.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# When there are true labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the knowledge of the ground truth class assignment, there are a number of metrics we can compute for clustering evaluation. I'll introduce the ones that are available in scikit learn modules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjusted Rand index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Adjusted Rand index is a function that measures the similarity of the two assignments ignoring permutations. It ranges from -1 to 1, with a value of 0 indicating random label assignment. Negative values are bad clustering results and positive values are good. A perfect match will result in a value of 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24242424242424246"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "labels_true = [0, 0, 0, 1, 1, 1]\n",
    "labels_pred = [0, 0, 1, 1, 2, 2]\n",
    "metrics.adjusted_rand_score(labels_true, labels_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mutual Information (MI) based scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Mutual Information is also a function that measures the agreement of the two assignments ignoring permutations. A perfect labeling will return a score of 1.0, while bad labeling will return negative scores, and random labeling 0.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2250422831983088"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "labels_true = [0, 0, 0, 1, 1, 1]\n",
    "labels_pred = [0, 0, 1, 1, 2, 2]\n",
    "\n",
    "metrics.adjusted_mutual_info_score(labels_true, labels_pred)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homogeneity, completeness and V-measure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the knowledge of the ground truth class assignments, we define homogeneity and completeness of clusters based on conditional entropy analysis:\n",
    "- homogeneity: each cluster contains only members of a single class\n",
    "- completeness: all members of a given class are assigned to the same cluster\n",
    "\n",
    "Both measures are bounded between 0.0 and 1.0, the higher the better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666669"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "labels_true = [0, 0, 0, 1, 1, 1]\n",
    "labels_pred = [0, 0, 1, 1, 2, 2]\n",
    "\n",
    "metrics.homogeneity_score(labels_true, labels_pred)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.420619835714305"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.completeness_score(labels_true, labels_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "V-measure is the harmonic mean of homogeneity and completeness:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5158037429793889"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.v_measure_score(labels_true, labels_pred) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we got a V-measure that is bad, we can look into the homogeneity and completeness scores to see what type of assignment mistakes there are. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fowlkes-Mallows scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Fowlkes-Mallows index is the geometric mean of the pairwise precision and recall. It ranges from 0.0 to 1.0, with a higher value indicating good results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4714045207910317"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "labels_true = [0, 0, 0, 1, 1, 1]\n",
    "labels_pred = [0, 0, 1, 1, 2, 2]\n",
    "\n",
    "metrics.fowlkes_mallows_score(labels_true, labels_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# When there aren't true labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the ground truth labels are not known, evaluations are performed on the clustered data itself. This is also called internal evaluation schemes. The aim is to find sets of clusters that are compact, with a small variance within clusters and big variance between clusters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Silhouette Coefficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Silhouette Coefficient for a single data point is defined by $$s=\\frac{b-a}{max(a,b)}$$ \n",
    "\n",
    "- a: the mean distance between the point and all other points in the same class\n",
    "- b: the mean distance between the point and all other points in the next nearest cluster \n",
    "\n",
    "The Silhouette Coefficient for a set of data points is the mean of the Silhouette Coefficient for each point. It is bounded between -1 and +1, with higher scores indicating denser and well-separated clustering. Scores around zero indicate overlapping clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5528190123564091"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn import datasets\n",
    "dataset = datasets.load_iris()\n",
    "X = dataset.data\n",
    "y = dataset.target\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "kmeans_model = KMeans(n_clusters=3, random_state=1).fit(X)\n",
    "labels = kmeans_model.labels_\n",
    "metrics.silhouette_score(X, labels, metric='euclidean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calinski-Harabaz Index (aka Variance Ratio Criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Calinski-Harabaz index is also known as the Variance Ratio Criterion, which is used to evaluate clustering results when the ground truth labels are unknown. A higher value indicates a model with better defined clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "561.62775662962"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn import datasets\n",
    "dataset = datasets.load_iris()\n",
    "X = dataset.data\n",
    "y = dataset.target\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "kmeans_model = KMeans(n_clusters=3, random_state=1).fit(X)\n",
    "labels = kmeans_model.labels_\n",
    "metrics.calinski_harabaz_score(X, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Davies-Bouldin Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Davies-Bouldin index is also used when the ground truth labels are unknown. A lower value indicates a model with better separation results. Zero is the lowest possible score, and values closer to zero indicates better partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6619715465007511"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "kmeans = KMeans(n_clusters=3, random_state=1).fit(X)\n",
    "labels = kmeans.labels_\n",
    "davies_bouldin_score(X, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Elbow method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because a good clustering result will produce clusters that are compact, we can use the within cluster sums of squares (also called the Inertia score) as a measure of model evaluation. Since the more clusters we have, likely the smaller the inertia we will get, there is a trade-off between number of clusters and minimizing inertia score. We want to find a best point where we have reasonable number of clusters and the minimum inertia possible. The Elbow method plots the total inertia against the number of clusters, and chose a number of clusters so that adding another cluster doesn't reduce the total inertia much (i.e., the bend in the plot, the 'elbow')."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auxiliary supervised task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One more method is to set up a supervised learning algorithm as an auxiliary task to evaluate the performance of the unsupervised clustering algorithm. For example, after the unsupervised clustering algorithm produces several clusters, we can use these clusters as latent variables to feed into a supervised classifier to perform some task that is related to the domain the data comes from. The performance of the supervised method can then be used as a proxy of the performance of the unsupervised learner. A more concrete example looks like this (borrowed from https://stats.stackexchange.com/questions/79028/performance-metrics-to-evaluate-unsupervised-learning):\n",
    "\n",
    "1. Learn representations of words using an unsupervised learner.\n",
    "2. Use the learned representations as input for a supervised learner performing some NLP task like parts of speech tagging or named entity recognition.\n",
    "3. Assess the performance of the unsupervised learner by its ability to improve the performance of the supervised learner compared to a baseline using a standard representation, like binary word presence features, as input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a conclusion, there is not and should not be a magic number that can summarize how well an unsupervised learning algorithm is performing without actually interpreting the results. Because \"how well a particular unsupervised method performs will largely depend on why one is doing unsupervised learning in the first place\"."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
